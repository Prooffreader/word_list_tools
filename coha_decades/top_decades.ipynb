{
 "metadata": {
  "name": "",
  "signature": "sha256:b9c3be73f51b82ad9a021c4953d70cc54d4e517199e0386da518f677da53fd60"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Top words per decade appeared"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import time\n",
      "from math import ceil\n",
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load pickle of all words and decades and remove those that appear in more than 10 decades\n",
      "\n",
      "df = pd.read_pickle(\"../data_user_pickle_csv/coha_1.pickle\")\n",
      "origlen = len(df)\n",
      "origwds = len(df.word.unique())\n",
      "df = df[df.nonalpha == False] # remove words with nonalphanumeric characters\n",
      "wordcount = pd.DataFrame(df.groupby('word').decade.count())\n",
      "wordcount = wordcount[wordcount.decade <= 10]\n",
      "df = df[df.word.isin(wordcount.index)]\n",
      "df = df[['word', 'decade', 'pct']]\n",
      "print \"{0} records reduced to {1} ({2:0.1f} %)\".format(origlen, len(df), len(df)*100.0/origlen)\n",
      "print \"{0} words reduced to {1} ({2:0.1f} %)\".format(origwds, len(df.word.unique()), \n",
      "                                                          len(df.word.unique())*100.0/origwds)\n",
      "print df.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2539728 records reduced to 894324 (35.2 %)\n",
        "436103 words reduced to 264885 (60.7 %)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       word  decade       pct\n",
        "113    aaaa    1990  0.000052\n",
        "114    aaaa    2000  0.000014\n",
        "115   aaaaa    1940  0.000004\n",
        "116   aaaaa    1970  0.000004\n",
        "117   aaaaa    2000  0.000042\n",
        "118  aaaaaa    1940  0.000004\n",
        "119  aaaaaa    1950  0.000004\n",
        "120  aaaaaa    1970  0.000004\n",
        "121  aaaaaa    1980  0.000004\n",
        "122  aaaaaa    2000  0.000004\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#keep words in crossword dictionary, e.g. not proper nouns\n",
      "\n",
      "origlen = len(df)\n",
      "origwds = len(df.word.unique())\n",
      "\n",
      "import json\n",
      "xwords = json.loads(open('../data_user_pickle_csv/coha_and_xword.json', 'r').read())\n",
      "df = df[df.word.isin(xwords)]\n",
      "\n",
      "print \"{0} records reduced to {1} ({2:0.1f} %)\".format(origlen, len(df), len(df)*100.0/origlen)\n",
      "print \"{0} words reduced to {1} ({2:0.1f} %)\".format(origwds, len(df.word.unique()), \n",
      "                                                          len(df.word.unique())*100.0/origwds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "894324 records reduced to 162734 (18.2 %)\n",
        "264885 words reduced to 29430 (11.1 %)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keep top 10000 words in terms of max and sum\n",
      "\n",
      "origlen = len(df)\n",
      "origwds = len(df.word.unique())\n",
      "\n",
      "dfsum = pd.DataFrame(df.groupby('word').pct.sum())\n",
      "dfsum.sort('pct', ascending=False, inplace=True)\n",
      "dfsum = dfsum[:10000]\n",
      "dfmax = pd.DataFrame(df.groupby('word').pct.max())\n",
      "dfmax.sort('pct', ascending=False, inplace=True)\n",
      "dfmax = dfmax[:10000]\n",
      "\n",
      "df = df[(df.word.isin(dfsum.index)) | (df.word.isin(dfmax.index))]\n",
      "\n",
      "print \"{0} records reduced to {1} ({2:0.1f} %)\".format(origlen, len(df), len(df)*100.0/origlen)\n",
      "print \"{0} words reduced to {1} ({2:0.1f} %)\".format(origwds, len(df.word.unique()), \n",
      "                                                          len(df.word.unique())*100.0/origwds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "162734 records reduced to 82548 (50.7 %)\n",
        "29430 words reduced to 11493 (39.1 %)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add sum per decade to dfsum\n",
      "\n",
      "series_count = df.groupby('word').decade.count()\n",
      "\n",
      "dfsum['pct_per_decade'] = 0.0\n",
      "\n",
      "for i in range(len(dfsum)):\n",
      "    dfsum.pct_per_decade.iloc[i] = (dfsum.pct[i] /\n",
      "                                    series_count[dfsum.index[i]])\n",
      "    \n",
      "dfsum.sort('pct_per_decade', ascending=False, inplace=True)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dfsum.head(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                  pct  pct_per_decade\n",
        "word                                 \n",
        "okay         0.063832        0.006383\n",
        "airport      0.031493        0.003499\n",
        "sutta        0.008483        0.002828\n",
        "fucking      0.013840        0.001977\n",
        "global       0.019391        0.001939\n",
        "brittles     0.001743        0.001743\n",
        "soviets      0.016589        0.001659\n",
        "computers    0.014902        0.001656\n",
        "fizgig       0.001648        0.001648\n",
        "almah        0.001642        0.001642\n",
        "fuck         0.014576        0.001620\n",
        "cloddy       0.009557        0.001593\n",
        "electronic   0.015746        0.001575\n",
        "airlines     0.013531        0.001503\n",
        "pengo        0.004263        0.001421\n",
        "vitamin      0.012656        0.001406\n",
        "coverage     0.013556        0.001356\n",
        "goddamn      0.012542        0.001254\n",
        "radar        0.009729        0.001216\n",
        "nazis        0.010591        0.001177\n",
        "airline      0.010704        0.001070\n",
        "reportedly   0.008322        0.001040\n",
        "kidding      0.010402        0.001040\n",
        "helicopter   0.010124        0.001012\n",
        "robot        0.009105        0.001012\n",
        "wildlife     0.008887        0.000987\n",
        "postwar      0.009799        0.000980\n",
        "fascist      0.008610        0.000861\n",
        "videos       0.002546        0.000849\n",
        "airplanes    0.008425        0.000842\n",
        "guidelines   0.006664        0.000833\n",
        "astronauts   0.005724        0.000818\n",
        "spacecraft   0.004890        0.000815\n",
        "programming  0.006517        0.000815\n",
        "electronics  0.006471        0.000809\n",
        "pickup       0.007922        0.000792\n",
        "motel        0.007507        0.000751\n",
        "sparkish     0.000743        0.000743\n",
        "medicare     0.004410        0.000735\n",
        "flanger      0.000725        0.000725\n",
        "teenage      0.005073        0.000725\n",
        "apiarian     0.000722        0.000722\n",
        "columnist    0.006486        0.000721\n",
        "teenagers    0.005039        0.000720\n",
        "manpower     0.006976        0.000698\n",
        "supermarket  0.004772        0.000682\n",
        "sego         0.005433        0.000679\n",
        "sexy         0.006053        0.000673\n",
        "cholesterol  0.006046        0.000672\n",
        "briefcase    0.006033        0.000670\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove words that are, after research, predominantly proper nouns:\n",
      "\n",
      "proper_nouns = ['sutta', 'soviets', 'almah', 'fizgig', 'pengo', 'sparkish', 'sego', 'cloddy']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load pickle of all words and decades and remove those that appear in more than 10 decades\n",
      "\n",
      "df_proper = pd.read_pickle(\"../data_user_pickle_csv/coha_1.pickle\")\n",
      "df_proper = df_proper[df_proper.nonalpha == False] # remove words with nonalphanumeric characters\n",
      "wordcount = pd.DataFrame(df_proper.groupby('word').decade.count())\n",
      "wordcount = wordcount[wordcount.decade <= 10]\n",
      "df_proper = df_proper[df_proper.word.isin(wordcount.index)]\n",
      "df_proper = df_proper[['word', 'decade', 'pct']]\n",
      "df_propersum = pd.DataFrame(df_proper.groupby('word').pct.sum())\n",
      "df_propersum.sort('pct', ascending=False, inplace=True)\n",
      "df_propersum = df_propersum[:10000]\n",
      "df_propermax = pd.DataFrame(df_proper.groupby('word').pct.max())\n",
      "df_propermax.sort('pct', ascending=False, inplace=True)\n",
      "df_propermax = df_propermax[:10000]\n",
      "df_proper = df_proper[(df_proper.word.isin(df_propersum.index)) | (df_proper.word.isin(df_propermax.index))]\n",
      "proper_series_count = df_proper.groupby('word').decade.count()\n",
      "df_propersum['pct_per_decade'] = 0.0\n",
      "for i in range(len(df_propersum)):\n",
      "    df_propersum.pct_per_decade.iloc[i] = (df_propersum.pct[i] /\n",
      "                                    proper_series_count[df_propersum.index[i]])\n",
      "df_propersum.sort('pct_per_decade', ascending=False, inplace=True)\n",
      "df_propersum = df_propersum[~df_propersum.index.isin(dfsum.index)]\n",
      "print df_propersum.head(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                 pct  pct_per_decade\n",
        "word                                \n",
        "dorriville  0.033207        0.033207\n",
        "altorf      0.042033        0.021016\n",
        "madiboo     0.018765        0.018765\n",
        "selico      0.018074        0.018074\n",
        "pacomo      0.016863        0.016863\n",
        "pufpace     0.016171        0.016171\n",
        "brazzo      0.015393        0.015393\n",
        "lescourt    0.013923        0.013923\n",
        "rossberg    0.027246        0.013623\n",
        "rheinthal   0.011415        0.011415\n",
        "plotwell    0.011242        0.011242\n",
        "immorina    0.010983        0.010983\n",
        "fourbin     0.010983        0.010983\n",
        "ridolpho    0.010810        0.010810\n",
        "demba       0.010118        0.010118\n",
        "bertocci    0.010118        0.010118\n",
        "torribal    0.009858        0.009858\n",
        "devalmore   0.009512        0.009512\n",
        "erlach      0.037984        0.009496\n",
        "lesc        0.009426        0.009426\n",
        "ploughby    0.009253        0.009253\n",
        "eberard     0.018079        0.009040\n",
        "makesafe    0.008994        0.008994\n",
        "ksenia      0.008994        0.008994\n",
        "joblin      0.017905        0.008952\n",
        "mentzikoff  0.008648        0.008648\n",
        "usaldo      0.008561        0.008561\n",
        "ubal        0.008475        0.008475\n",
        "almeyda     0.016175        0.008088\n",
        "hippolito   0.015397        0.007699\n",
        "barogo      0.007610        0.007610\n",
        "beraldo     0.015061        0.007531\n",
        "hardrun     0.007523        0.007523\n",
        "arandez     0.007351        0.007351\n",
        "maillac     0.007091        0.007091\n",
        "mahadi      0.007091        0.007091\n",
        "bloomville  0.028237        0.007059\n",
        "spendall    0.007005        0.007005\n",
        "lanissa     0.006659        0.006659\n",
        "spicket     0.006287        0.006287\n",
        "ridol       0.006226        0.006226\n",
        "shenac      0.006137        0.006137\n",
        "rainouard   0.006053        0.006053\n",
        "flaurence   0.005967        0.005967\n",
        "wildenhain  0.017735        0.005912\n",
        "cerval      0.011765        0.005883\n",
        "oresca      0.005794        0.005794\n",
        "quicksite   0.005448        0.005448\n",
        "darina      0.005362        0.005362\n",
        "chetwynde   0.005292        0.005292\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "decades = range(1810, 2010, 10)\n",
      "dftop = dfsum[:50]\n",
      "dftoplookup = df[df.word.isin(keep)]\n",
      "for decade in decades:\n",
      "    dftop[decade] = 0.0\n",
      "for i in range(len(dftop)):\n",
      "    for decade in decades:\n",
      "        if len(dftoplookup[(dftoplookup.word == dftop.index[i]) &\n",
      "                           (dftoplookup.decade == decade)]) > 0:\n",
      "            dftop[decade].iloc[i] = dftoplookup[(dftoplookup.word == dftop.index[i]) &\n",
      "                                                (dftoplookup.decade == decade)].pct.iloc[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dftop.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              pct  pct_per_decade      1810  1820  1830  1840  1850  1860  \\\n",
        "word                                                                        \n",
        "okay     0.063832        0.006383  0.000000     0     0     0     0     0   \n",
        "airport  0.031493        0.003499  0.000000     0     0     0     0     0   \n",
        "sutta    0.008483        0.002828  0.008475     0     0     0     0     0   \n",
        "fucking  0.013840        0.001977  0.000000     0     0     0     0     0   \n",
        "global   0.019391        0.001939  0.000000     0     0     0     0     0   \n",
        "\n",
        "         1870  1880  ...   1910      1920      1930      1940      1950  \\\n",
        "word                 ...                                                  \n",
        "okay        0     0  ...      0  0.000008  0.001353  0.003058  0.004406   \n",
        "airport     0     0  ...      0  0.000366  0.001294  0.002265  0.003179   \n",
        "sutta       0     0  ...      0  0.000000  0.000000  0.000000  0.000004   \n",
        "fucking     0     0  ...      0  0.000000  0.000071  0.000000  0.000042   \n",
        "global      0     0  ...      0  0.000028  0.000017  0.000526  0.000548   \n",
        "\n",
        "             1960      1970      1980      1990      2000  \n",
        "word                                                       \n",
        "okay     0.006313  0.008459  0.009622  0.014044  0.016564  \n",
        "airport  0.005269  0.005690  0.005091  0.004271  0.004069  \n",
        "sutta    0.000000  0.000000  0.000004  0.000000  0.000000  \n",
        "fucking  0.000755  0.003169  0.002874  0.003647  0.003281  \n",
        "global   0.000716  0.001191  0.002196  0.005896  0.008268  \n",
        "\n",
        "[5 rows x 22 columns]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dftop.iloc[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pct               0.008483\n",
        "pct_per_decade    0.002828\n",
        "1810              0.008475\n",
        "1820              0.000000\n",
        "1830              0.000000\n",
        "1840              0.000000\n",
        "1850              0.000000\n",
        "1860              0.000000\n",
        "1870              0.000000\n",
        "1880              0.000000\n",
        "1890              0.000000\n",
        "1900              0.000000\n",
        "1910              0.000000\n",
        "1920              0.000000\n",
        "1930              0.000000\n",
        "1940              0.000000\n",
        "1950              0.000004\n",
        "1960              0.000000\n",
        "1970              0.000000\n",
        "1980              0.000004\n",
        "1990              0.000000\n",
        "2000              0.000000\n",
        "Name: sutta, dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "pattern_search.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# word_list_tools repo\n",
      "\n",
      "by David Taylor, www.prooffreader.com, prooffreader@gmail.com\n",
      "\n",
      "a collection of tools to create and analyze lists of words using python with pandas and matplotlib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## pattern_search ##\n",
      "\n",
      "starting with a word list, find words that match (1) a predefined pattern, or (2) a pattern based on other words in the database -- an O(mn) operation, so be careful.\n",
      "\n",
      "Word list is pandas dataframe with columns 'word' and 'freq'. Any other columns will be ignored.\n",
      "\n",
      "** initial_data_munge must be run first to create pickled dataframes of word lists **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "dataframe_base = 'coha_words' # change as needed to point to pickle\n",
      "dataframe_description = 'Brown Corpus from Natural Language Toolkit'\n",
      "#dataframe_description = 'Corpus of Historical American English, Brigham Young University (Academic license)'\n",
=======
      "dataframe_base = 'brown_words' # change as needed to point to pickle\n",
      "dataframe_description = 'Brown Corpus from Natural Language Toolkit'\n",
>>>>>>> e11bf7d9fa40e5774591ba1183b1806eba8d5c1a
      "data_path = 'data'\n",
      "\n",
      "import pandas as pd\n",
      "import os\n",
<<<<<<< HEAD
=======
      "import re\n",
      "import time\n",
>>>>>>> e11bf7d9fa40e5774591ba1183b1806eba8d5c1a
      "\n",
      "nb_path = 'pattern_search'\n",
      "\n",
      "words = pd.read_pickle(data_path + \"/\" + dataframe_base + \".pickle\")\n",
      "\n",
      "if not os.path.exists(nb_path):\n",
      "    os.mkdir(nb_path)\n",
<<<<<<< HEAD
      "os.chdir(nb_path)\n",
=======
>>>>>>> e11bf7d9fa40e5774591ba1183b1806eba8d5c1a
      "\n",
      "def streamline(df):           # function to streamline dataframe right before search to speed it up\n",
      "    df = df[['word', 'freq']]"
     ],
     "language": "python",
     "metadata": {},
<<<<<<< HEAD
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'data/coha_words.pickle'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-9787b52cf9d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnb_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pattern_search'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdataframe_base\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\David\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\pickle.pyc\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/coha_words.pickle'"
       ]
      }
     ],
     "prompt_number": 4
=======
     "outputs": [],
     "prompt_number": 39
>>>>>>> e11bf7d9fa40e5774591ba1183b1806eba8d5c1a
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### search for single pattern"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_single, single_suffix = (False, '_search1')\n",
      "\n",
      "# Create copy of dataframe and subset it as desired\n",
      "\n",
      "df_single = words.copy()\n",
<<<<<<< HEAD
      "print len(df_single)\n",
      "\n",
      "df_single = df_single[df_single.freq > 20]\n",
      "print len(df_single)"
=======
      "\n",
      "df_single = df_single[df_single.freq > 20]\n",
      "\n",
      "df_single = df_single[df_single.word.str.contains(\"^a\")] #str.contains uses regex\n",
      "\n",
      "print \"%d subsetted\" % (len(df_single))\n",
      "\n",
      "print df_single.head(3)\n",
      "print \"...\"\n",
      "print df_single.tail(3)\n",
      "\n",
      "if save_single == True:\n",
      "    df_single.to_csv(nb_path + \"/\" + dataframe_base + single_suffix + '.csv')"
>>>>>>> e11bf7d9fa40e5774591ba1183b1806eba8d5c1a
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
<<<<<<< HEAD
        "40234\n",
        "4617\n"
       ]
      }
     ],
     "prompt_number": 3
=======
        "322 subsetted\n",
        "     word   freq  length\n",
        "1341  and  28853       3\n",
        "0       a  23195       1\n",
        "2032   as   7253       2\n",
        "...\n",
        "             word  freq  length\n",
        "197     accepting    21       9\n",
        "194    acceptable    21      10\n",
        "1441  anniversary    21      11\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### search for pattern from one word within another within same dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for this example, we will find words containing the letter e within other words that end in a vowel,\n",
      "# only if the smaller word does not occur at the very beginning of the larger word.\n",
      "\n",
      "save_double, double_suffix = (False, '_double1')\n",
      "\n",
      "df_double_to_search = words.copy()\n",
      "start_time = time.time()\n",
      "\n",
      "#if filter before search is required, do it here.\n",
      "print \"%d before subset\" % (len(df_double_to_search))\n",
      "df_double_to_search = df_double_to_search[df_double_to_search.freq > 30]\n",
      "print \"%d subsetted\" % (len(df_double_to_search))\n",
      "print df_double_to_search.head(3)\n",
      "print \"...\"\n",
      "print df_double_to_search.tail(3)\n",
      "\n",
      "wordlist_search = df_double_to_search.word.tolist()\n",
      "wordlist_criteria = df_double_to_search.word.tolist()\n",
      "\n",
      "#subset wordlist_search\n",
      "wordlist_search = [word for word in wordlist_search if re.search('[aeiouy]$', word)] # see example below if you don't want to use regex\n",
      "print '\\n%d words in search:' % (len(wordlist_search)),\n",
      "print wordlist_search[:3], '...', wordlist_search[-3:]\n",
      "\n",
      "#subset wordlist_criteria\n",
      "wordlist_criteria = [word for word in wordlist_criteria if word.find('e') != -1] # could have used re.search('e', word)\n",
      "print '%d words in criteria:' % (len(wordlist_criteria)),\n",
      "print wordlist_criteria[:3], '...', wordlist_criteria[-3:]\n",
      "\n",
      "#remove words in wordlist_search from wordlist_criteria if desired\n",
      "wordlist_criteria = [word for word in wordlist_criteria if word not in wordlist_search]\n",
      "print '%d words in criteria:' % (len(wordlist_criteria)),\n",
      "print wordlist_criteria[:3], '...', wordlist_criteria[-3:]\n",
      "\n",
      "ten_percent_range = len(wordlist_search) * len(wordlist_criteria) / 10\n",
      "result_search = []\n",
      "result_criteria = []\n",
      "counter = 0\n",
      "countdown = 10\n",
      "\n",
      "\n",
      "print '\\nCountdown: 10 ...',\n",
      "for word_searched in wordlist_search:\n",
      "    for word_crit in wordlist_criteria:\n",
      "        counter += 1\n",
      "        if counter % ten_percent_range == 0:\n",
      "            countdown -= 1\n",
      "            print countdown,\n",
      "            print '...',\n",
      "        \n",
      "        # here is the final criteria for positive result\n",
      "        if word_searched.find(word_crit) > 0: # -1 means not found, 0 means word_crit found at beginning of word_searched\n",
      "            result_search.append(word_searched)\n",
      "            result_criteria.append(word_crit)\n",
      "print \"Done.\"\n",
      "elapsed_time = (time.time() - start_time) / 60\n",
      "print \"Elapsed time: %0.2f minutes.\\n\" % (elapsed_time)\n",
      "            \n",
      "df_double = pd.DataFrame({'word_searched': result_search, 'word_criteria': result_criteria})\n",
      "\n",
      "print \"%d matches found\" % (len(df_double))\n",
      "\n",
      "print df_double.head(3)\n",
      "print \"...\"\n",
      "print df_double.tail(3)\n",
      "            \n",
      "if save_double == True:\n",
      "    df_double.to_csv(nb_path + \"/\" + dataframe_base + double_suffix + '.csv')      \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40234 before subset\n",
        "3379 subsetted\n",
        "      word   freq  length\n",
        "35901  the  69971       3\n",
        "24581   of  36412       2\n",
        "1341   and  28853       3\n",
        "...\n",
        "           word  freq  length\n",
        "37024  troubled    31       8\n",
        "33964    starts    31       6\n",
        "8914    dancers    31       7\n",
        "\n",
        "903 words in search: ['the', 'to', 'a'] ... ['consequently', 'lightly', 'dave']\n",
        "2097 words in criteria: ['the', 'he', 'be'] ... ['dave', 'troubled', 'dancers']\n",
        "1401 words in criteria:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['her', 'their', 'been'] ... ['considerations', 'troubled', 'dancers']\n",
        "\n",
        "Countdown: 10 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 ... Done.\n",
        "Elapsed time: 0.01 minutes.\n",
        "\n",
        "52 matches found\n",
        "  word_criteria word_searched\n",
        "0           her         there\n",
        "1           her         where\n",
        "2           per    experience\n",
        "...\n",
        "   word_criteria word_searched\n",
        "49          even       revenue\n",
        "50           ten      sentence\n",
        "51           her       thereby\n"
       ]
      }
     ],
     "prompt_number": 48
>>>>>>> e11bf7d9fa40e5774591ba1183b1806eba8d5c1a
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
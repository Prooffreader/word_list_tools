{
 "metadata": {
  "name": "initial_data_munge.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# word_list_tools repo\n",
      "\n",
      "by David Taylor, www.prooffreader.com, prooffreader@gmail.com\n",
      "\n",
      "a collection of tools to create and analyze lists of words using python with pandas and matplotlib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## initial_data_munge\n",
      "\n",
      "Creates complete and summary dataframes\n",
      "\n",
      "Two corpus databases/word lists implemented:\n",
      "\n",
      "    1. COHA (not public domain, requires license to use)\n",
      "    2. Brown corpus (public domain, part of the python Natural Language Toolkit, NLTK).\n",
      "    \n",
      "The scripts should be easily adaptable to other lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### COHA ###\n",
      "\n",
      "Uses the Corpus of Historical American English 1-gram corpus from Brigham Young University, available for a fee or with academic license from http://corpus.byu.edu/coha/\n",
      "\n",
      "Only small snippets of up to five records at a time are shown as examples in this repo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "\n",
      "data_path = 'data'\n",
      "data_filename = '1_pos_n_cs_n.txt' # Without part of speech tagging or upper-/lowercase differentiation "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create \"coha\" initial dataframe\n",
      "\n",
      "To do: add column for decade range, e.g. 1910-1940"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_initial = True # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/coha.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_initial == True: # if pickle already exists, coha is not processed, only read\n",
      "    print \"Processing \" + coha_filename\n",
      "    coha = pd.read_table(data_path + \"/\" + data_filename)\n",
      "    print \"\\nTail of initial dataframe:\"\n",
      "    print coha.tail()\n",
      "    coha.columns = ['freq', 'word', 'decade']\n",
      "    coha = coha[['word', 'freq', 'decade']]\n",
      "    coha = coha.fillna('nan')\n",
      "    coha['nonalpha'] = False\n",
      "    coha['nonalpha'][coha.word.str.contains('[^A-Za-z]')] = True\n",
      "    coha['length'] = 0\n",
      "    decades = [1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900,\n",
      "               1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000]\n",
      "    for idx, row in coha.iterrows():  ## this step take 2-3 minutes on a medium-quality desktop computer ca 2013\n",
      "        coha['decade'][idx] = decades[coha['decade'][idx] - 1]\n",
      "        coha['length'][idx] = len(str(coha.word[idx]))\n",
      "    def coha_pct(group):\n",
      "        freq = group.freq.astype(float)\n",
      "        group['pct'] = (freq / freq.sum() * 100)\n",
      "        return group\n",
      "    coha = coha.groupby(['decade']).apply(coha_pct)\n",
      "    coha.to_pickle(pickle_full_path)\n",
      "    \n",
      "else:\n",
      "    print \"Reading \" + pickle_full_path\n",
      "    coha = pd.read_pickle(pickle_full_path)\n",
      "    \n",
      "print \"\\nTail of dataframe:\"\n",
      "print coha.tail()\n",
      "\n",
      "# note: 'nonalpha' is True if word contains any characters but a-z or A-Z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading data/coha.pickle\n",
        "\n",
        "Tail of dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              word  freq  decade nonalpha  length       pct\n",
        "2539723       zzzz     3    2000    False       4  0.000011\n",
        "2539724     zzzzzz     1    1980    False       6  0.000004\n",
        "2539725     zzzzzz     1    2000    False       6  0.000004\n",
        "2539726   zzzzzzzz     3    2000    False       8  0.000011\n",
        "2539727  zzzzzzzzz     3    2000    False       9  0.000011\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Creates dataframe coha_words that ignores tokens with non-alphabetic characters and consolidates all decades into one record"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_words = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/coha_words.pickle\"\n",
      "mean_median_full_path = data_path + \"coha_words_mean_median.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_words == True:\n",
      "    print 'Creating dataframe.'\n",
      "    total_freq = 0\n",
      "    weighted_length = 0\n",
      "    coha_words = coha[coha.nonalpha == 0]\n",
      "    coha_words = pd.DataFrame(coha_words.groupby(['word']).sum()).reset_index(drop=False)\n",
      "    coha_words.columns = ['word', 'freq', 'erase1', 'erase2', 'decadesxlen', 'erase3']\n",
      "    coha_words['length'] = 0\n",
      "    coha_words['decades'] = 0\n",
      "    coha_words.sort('freq', ascending = False, inplace=True)\n",
      "    total_freq = coha_words.freq.sum()\n",
      "    median_counter = total_freq / 2\n",
      "    median_found = False\n",
      "    for idx, row in coha_words.iterrows():\n",
      "        curr_len = len(coha_words.word[idx])\n",
      "        coha_words.length[idx] = curr_len\n",
      "        coha_words.decades[idx] = coha_words.decadesxlen[idx] / coha_words.length[idx]\n",
      "        weighted_length += coha_words.freq[idx] * curr_len\n",
      "        median_counter -= curr_len * coha_words.freq[idx]\n",
      "        if median_counter < 0 and median_found == False:\n",
      "            wt_median_len = curr_len\n",
      "            median_found = True\n",
      "    coha_words = coha_words[['word', 'freq', 'length', 'decades']]\n",
      "    coha_words.to_pickle(pickle_full_path)\n",
      "    wt_mean_len = weighted_length * 1.0 / total_freq\n",
      "    pickle.dump((wt_mean_len, wt_median_len), open(mean_median_full_path, \"w\"))\n",
      "    \n",
      "else:\n",
      "    print 'Reading pickle.'\n",
      "    coha_words = pd.read_pickle(pickle_full_path)\n",
      "    wt_mean_len, wt_median_len = pickle.load(open(mean_median_full_path, \"r\"))\n",
      "    \n",
      "print \"Weighted mean word length: \" + str(wt_mean_len)\n",
      "print \"Weighted median word length: \" + str(wt_median_len)\n",
      "print \"Tail of coha_words dataframe:\"\n",
      "print coha_words.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating dataframe.\n",
        "Weighted mean word length: 4.45220167708"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Weighted median word length: 4\n",
        "Tail of coha_words dataframe:\n",
        "            word  freq  length  decades\n",
        "336193      zieg     1       4        1\n",
        "87532   eckerson     1       8        1\n",
        "125546   haldore     1       7        1\n",
        "303578    tranio     1       6        1\n",
        "40616       burz     1       4        1\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create file 'coha_decades_top.txt' which shows the top ten words for each number of decades"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not os.path.isfile(data_path + '/' + 'coha_decades_top.txt'):\n",
      "    # for each number of decades, find the top 10 words\n",
      "    f = open(data_path + '/' + 'coha_decades_top.txt', 'a')\n",
      "    for dec in range(1,21):\n",
      "        print >> f, \"%d decade(s)\" % dec\n",
      "        print >> f, coha_words[coha_words.decades == dec].sort('freq', ascending=False).head(10)\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Brown corpus from NLTK 3##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named nltk",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-22-3caeba1b80e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named nltk"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
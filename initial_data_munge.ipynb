{
 "metadata": {
  "name": "initial_data_munge.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# word_list_tools repo\n",
      "\n",
      "by David Taylor, www.prooffreader.com, prooffreader@gmail.com\n",
      "\n",
      "a collection of tools to create and analyze lists of words using python with pandas and matplotlib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## initial_data_munge\n",
      "\n",
      "Creates complete and summary dataframes\n",
      "\n",
      "Two corpus databases/word lists implemented:\n",
      "\n",
      "    1. COHA (not public domain, requires license to use)\n",
      "    2. Brown corpus (public domain, part of the python Natural Language Toolkit, NLTK).\n",
      "    \n",
      "The scripts should be easily adaptable to other lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### COHA ###\n",
      "\n",
      "Uses the Corpus of Historical American English 1-gram corpus from Brigham Young University, available for a fee or with academic license from http://corpus.byu.edu/coha/\n",
      "\n",
      "Only small snippets of up to five records at a time are shown as examples in this repo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "\n",
      "data_path = 'data'\n",
      "coha_filename = '1_pos_n_cs_n.txt' # Without part of speech tagging or upper-/lowercase differentiation "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create \"coha\" initial dataframe\n",
      "\n",
      "To do: add column for decade range, e.g. 1910-1940"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_initial = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/coha.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_initial == True: # if pickle already exists, coha is not processed, only read\n",
      "    print \"Processing \" + coha_filename\n",
      "    coha = pd.read_table(data_path + \"/\" + coha_filename)\n",
      "    print \"\\nTail of initial dataframe:\"\n",
      "    print coha.tail()\n",
      "    coha.columns = ['freq', 'word', 'decade']\n",
      "    coha = coha[['word', 'freq', 'decade']]\n",
      "    coha = coha.fillna('nan')\n",
      "    coha['nonalpha'] = False\n",
      "    coha['nonalpha'][coha.word.str.contains('[^A-Za-z]')] = True\n",
      "    coha['length'] = 0\n",
      "    decades = [1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900,\n",
      "               1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000]\n",
      "    for idx, row in coha.iterrows():  ## this step take 2-3 minutes on a medium-quality desktop computer ca 2013\n",
      "        coha['decade'][idx] = decades[coha['decade'][idx] - 1]\n",
      "        coha['length'][idx] = len(str(coha.word[idx]))\n",
      "    def coha_pct(group):\n",
      "        freq = group.freq.astype(float)\n",
      "        group['pct'] = (freq / freq.sum() * 100)\n",
      "        return group\n",
      "    coha = coha.groupby(['decade']).apply(coha_pct)\n",
      "    coha.to_pickle(pickle_full_path)\n",
      "    \n",
      "else:\n",
      "    print \"Reading \" + pickle_full_path\n",
      "    coha = pd.read_pickle(pickle_full_path)\n",
      "    \n",
      "print \"\\nTail of dataframe:\"\n",
      "print coha.tail()\n",
      "\n",
      "# note: 'nonalpha' is True if word contains any characters but a-z or A-Z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing 1_pos_n_cs_n.txt\n",
        "\n",
        "Tail of initial dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         freq    word-cs  decade\n",
        "2539723     3       zzzz      20\n",
        "2539724     1     zzzzzz      18\n",
        "2539725     1     zzzzzz      20\n",
        "2539726     3   zzzzzzzz      20\n",
        "2539727     3  zzzzzzzzz      20\n",
        "\n",
        "Tail of dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              word  freq  decade nonalpha  length       pct\n",
        "2539723       zzzz     3    2000    False       4  0.000011\n",
        "2539724     zzzzzz     1    1980    False       6  0.000004\n",
        "2539725     zzzzzz     1    2000    False       6  0.000004\n",
        "2539726   zzzzzzzz     3    2000    False       8  0.000011\n",
        "2539727  zzzzzzzzz     3    2000    False       9  0.000011\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Creates dataframe coha_words that ignores tokens with non-alphabetic characters and consolidates all decades into one record"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_words = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/coha_words.pickle\"\n",
      "mean_median_full_path = data_path + \"/\" + \"coha_words_mean_median.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_words == True:\n",
      "    print 'Creating dataframe.'\n",
      "    total_freq = 0\n",
      "    weighted_length = 0\n",
      "    coha_words = coha[coha.nonalpha == 0]\n",
      "    coha_words = pd.DataFrame(coha_words.groupby(['word']).sum()).reset_index(drop=False)\n",
      "    coha_words.columns = ['word', 'freq', 'erase1', 'erase2', 'decadesxlen', 'erase3']\n",
      "    coha_words['length'] = 0\n",
      "    coha_words['decades'] = 0\n",
      "    coha_words.sort('freq', ascending = False, inplace=True)\n",
      "    total_freq = coha_words.freq.sum()\n",
      "    median_counter = total_freq / 2\n",
      "    median_found = False\n",
      "    for idx, row in coha_words.iterrows():\n",
      "        curr_len = len(coha_words.word[idx])\n",
      "        coha_words.length[idx] = curr_len\n",
      "        coha_words.decades[idx] = coha_words.decadesxlen[idx] / coha_words.length[idx]\n",
      "        weighted_length += coha_words.freq[idx] * curr_len\n",
      "        median_counter -= curr_len * coha_words.freq[idx]\n",
      "        if median_counter < 0 and median_found == False:\n",
      "            wt_median_len = curr_len\n",
      "            median_found = True\n",
      "    coha_words = coha_words[['word', 'freq', 'length', 'decades']]\n",
      "    coha_words.to_pickle(pickle_full_path)\n",
      "    wt_mean_len = weighted_length * 1.0 / total_freq\n",
      "    pickle.dump((wt_mean_len, wt_median_len), open(mean_median_full_path, \"w\"))\n",
      "    \n",
      "else:\n",
      "    print 'Reading pickle.'\n",
      "    coha_words = pd.read_pickle(pickle_full_path)\n",
      "    wt_mean_len, wt_median_len = pickle.load(open(mean_median_full_path, \"r\"))\n",
      "    \n",
      "print \"Weighted mean word length: \" + str(wt_mean_len)\n",
      "print \"Weighted median word length: \" + str(wt_median_len)\n",
      "print \"Tail of coha_words dataframe:\"\n",
      "print coha_words.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating dataframe.\n",
        "Weighted mean word length: 4.45220167708"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Weighted median word length: 4\n",
        "Tail of coha_words dataframe:\n",
        "            word  freq  length  decades\n",
        "336193      zieg     1       4        1\n",
        "87532   eckerson     1       8        1\n",
        "125546   haldore     1       7        1\n",
        "303578    tranio     1       6        1\n",
        "40616       burz     1       4        1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create file 'coha_decades_top.txt' which shows the top ten words for each number of decades"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not os.path.isfile(data_path + '/' + 'coha_decades_top.txt'):\n",
      "    # for each number of decades, find the top 10 words\n",
      "    f = open(data_path + '/' + 'coha_decades_top.txt', 'a')\n",
      "    for dec in range(1,21):\n",
      "        print >> f, \"%d decade(s)\" % dec\n",
      "        print >> f, coha_words[coha_words.decades == dec].sort('freq', ascending=False).head(10)\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Brown corpus from NLTK 3##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "#nltk.download() # uncomment to start downloader window to fetch brown or other corpora\n",
      "\n",
      "data_path = 'data'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_brown_initial = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/brown_df.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_brown_initial == True: \n",
      "    \n",
      "    print \"Processing Brown corpus from NLTK\"\n",
      "    categories = []\n",
      "    words = []\n",
      "    frequencies = []\n",
      "    \n",
      "    for category in brown.categories():\n",
      "        wordlist = brown.words(categories=category)\n",
      "        freqs = nltk.FreqDist([w.lower() for w in wordlist])\n",
      "        for key in freqs.keys():\n",
      "            categories.append(category)\n",
      "            words.append(key)\n",
      "            frequencies.append(freqs[key])\n",
      "    \n",
      "    brown_df = pd.DataFrame({'word':words, 'freq':frequencies, 'category':categories})\n",
      "    brown_df['nonalpha'] = False\n",
      "    brown_df['nonalpha'][brown_df.word.str.contains('[^A-Za-z]')] = True\n",
      "    brown_df.to_pickle(pickle_full_path)\n",
      "    \n",
      "else:\n",
      "    print \"Reading \" + pickle_full_path\n",
      "    brown_df = pd.read_pickle(pickle_full_path)\n",
      "    \n",
      "print \"\\nTail of dataframe:\"\n",
      "print brown_df.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing Brown corpus from NLTK\n",
        "\n",
        "Tail of dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               category  freq        word nonalpha\n",
        "139440  science_fiction     1        yoga    False\n",
        "139441  science_fiction     1      you'll     True\n",
        "139442  science_fiction     1    yourself    False\n",
        "139443  science_fiction     1  zigzagging    False\n",
        "139444  science_fiction     1        zone    False\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "build dataframe of frequencies of unique words, consolidating categories, discarding tokens with non-alphabetic characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_brown_words = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/brown_words.pickle\"\n",
      "\n",
      "mean_median_full_path = data_path + \"/\" + \"brown_words_mean_median.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_brown_words == True:\n",
      "    print 'Creating dataframe.'\n",
      "    total_freq = 0\n",
      "    weighted_length = 0\n",
      "    brown_words = brown_df[brown_df.nonalpha == 0]\n",
      "    brown_words = pd.DataFrame(brown_words.groupby(['word']).sum()).reset_index(drop=False)\n",
      "    brown_words['length'] = 0\n",
      "    brown_words.sort('freq', ascending = False, inplace=True)\n",
      "    total_freq = brown_words.freq.sum()\n",
      "    median_counter = total_freq / 2\n",
      "    median_found = False\n",
      "    for idx, row in brown_words.iterrows():\n",
      "        curr_len = len(brown_words.word[idx])\n",
      "        brown_words.length[idx] = curr_len\n",
      "        weighted_length += brown_words.freq[idx] * curr_len\n",
      "        median_counter -= curr_len * brown_words.freq[idx]\n",
      "        if median_counter < 0 and median_found == False:\n",
      "            wt_median_len = curr_len\n",
      "            median_found = True\n",
      "    brown_words = brown_words[['word', 'freq', 'length']]\n",
      "    brown_words.to_pickle(pickle_full_path)\n",
      "    wt_mean_len = weighted_length * 1.0 / total_freq\n",
      "    pickle.dump((wt_mean_len, wt_median_len), open(mean_median_full_path, \"w\"))\n",
      "    \n",
      "else:\n",
      "    print 'Reading pickle.'\n",
      "    brown_words = pd.read_pickle(pickle_full_path)\n",
      "    wt_mean_len, wt_median_len = pickle.load(open(mean_median_full_path, \"r\"))\n",
      "    \n",
      "print \"Weighted mean word length: \" + str(wt_mean_len)\n",
      "print \"Weighted median word length: \" + str(wt_median_len)\n",
      "print \"Tail of brown_words dataframe:\"\n",
      "print brown_words.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating dataframe.\n",
        "Weighted mean word length: 4.68324851586"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Weighted median word length: 4\n",
        "Tail of brown_words dataframe:\n",
        "                   word  freq  length\n",
        "11805           enchant     1       7\n",
        "11804         enchained     1       9\n",
        "11803  encephalographic     1      16\n",
        "11802      encephalitis     1      12\n",
        "29339           regalia     1       7\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
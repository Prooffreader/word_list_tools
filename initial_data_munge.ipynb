{
 "metadata": {
  "name": "",
  "signature": "sha256:ad02b47b0ba83a82fc2362efe2b1e00a9a94113df6695559f46e18ae963c0fda"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# word_list_tools repo\n",
      "\n",
      "by David Taylor, www.prooffreader.com, prooffreader@gmail.com\n",
      "\n",
      "a collection of tools to create and analyze lists of words using python with pandas and matplotlib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## initial_data_munge\n",
      "\n",
      "Creates complete and summary dataframes\n",
      "\n",
      "Three corpus databases/word lists implemented:\n",
      "\n",
      "    1. COHA (not public domain for single word frequencies, requires license to use)\n",
      "    2. Brown corpus (public domain, part of the python Natural Language Toolkit, NLTK).\n",
      "    3. Europarl: A Parallel Corpus for Statistical Machine Translation, Philipp Koehn, MT Summit 2005 (http://statmt.org/europarl/)\n",
      "    \n",
      "The scripts should be easily adaptable to other lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### COHA ###\n",
      "\n",
      "Uses the Corpus of Historical American English 1-gram corpus from Brigham Young University, available for a fee or with academic license from http://corpus.byu.edu/coha/\n",
      "\n",
      "Only small snippets of up to five records at a time are shown as examples in this repo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "import json\n",
      "\n",
      "init_path = 'data_initial/'\n",
      "data_path = 'data_user_pickle_csv/'\n",
      "coha_filename = 'coha_1_pos_n_cs_n.txt' # Without part of speech tagging or upper-/lowercase differentiation "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create \"coha\" initial dataframe\n",
      "\n",
      "To do: add column for decade range, e.g. 1910-1940 if word was only used within those decades"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_initial = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"coha_1.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_initial == True: # if pickle already exists, coha is not processed, only read\n",
      "    print \"Processing \" + coha_filename\n",
      "    coha = pd.read_table(init_path + coha_filename)\n",
      "    print \"\\nTail of initial dataframe:\"\n",
      "    print coha.tail()\n",
      "    coha.columns = ['freq', 'word', 'decade']\n",
      "    coha = coha[['word', 'freq', 'decade']]\n",
      "    coha = coha.fillna('nan')\n",
      "    coha['nonalpha'] = False\n",
      "    coha['nonalpha'][coha.word.str.contains('[^A-Za-z]')] = True\n",
      "    coha['length'] = 0\n",
      "    decades = [1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900,\n",
      "               1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000]\n",
      "    for idx, row in coha.iterrows():  ## this step take 2-3 minutes on a medium-quality desktop computer ca 2013\n",
      "        coha['decade'][idx] = decades[coha['decade'][idx] - 1]\n",
      "        coha['length'][idx] = len(str(coha.word[idx]))\n",
      "    def coha_pct(group):\n",
      "        freq = group.freq.astype(float)\n",
      "        group['pct'] = (freq / freq.sum() * 100)\n",
      "        return group\n",
      "    coha = coha.groupby(['decade']).apply(coha_pct)\n",
      "    coha.to_pickle(pickle_full_path)\n",
      "    \n",
      "else:\n",
      "    print \"Reading \" + pickle_full_path\n",
      "    coha = pd.read_pickle(pickle_full_path)\n",
      "    \n",
      "print \"\\nTail of dataframe:\"\n",
      "print coha.tail()\n",
      "\n",
      "# note: 'nonalpha' is True if word contains any characters but a-z or A-Z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing 1_pos_n_cs_n.txt\n",
        "\n",
        "Tail of initial dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         freq    word-cs  decade\n",
        "2539723     3       zzzz      20\n",
        "2539724     1     zzzzzz      18\n",
        "2539725     1     zzzzzz      20\n",
        "2539726     3   zzzzzzzz      20\n",
        "2539727     3  zzzzzzzzz      20\n",
        "\n",
        "Tail of dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              word  freq  decade nonalpha  length       pct\n",
        "2539723       zzzz     3    2000    False       4  0.000011\n",
        "2539724     zzzzzz     1    1980    False       6  0.000004\n",
        "2539725     zzzzzz     1    2000    False       6  0.000004\n",
        "2539726   zzzzzzzz     3    2000    False       8  0.000011\n",
        "2539727  zzzzzzzzz     3    2000    False       9  0.000011\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Creates dataframe coha_words that ignores tokens with non-alphabetic characters and consolidates all decades into one record"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_words = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"coha_words.pickle\"\n",
      "mean_median_full_path = data_path + \"coha_words_mean_median.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_words == True:\n",
      "    print 'Creating dataframe.'\n",
      "    total_freq = 0\n",
      "    weighted_length = 0\n",
      "    coha_words = coha[coha.nonalpha == 0]\n",
      "    coha_words = pd.DataFrame(coha_words.groupby(['word']).sum()).reset_index(drop=False)\n",
      "    coha_words.columns = ['word', 'freq', 'erase1', 'erase2', 'decadesxlen', 'erase3']\n",
      "    coha_words['length'] = 0\n",
      "    coha_words['decades'] = 0\n",
      "    coha_words.sort('freq', ascending = False, inplace=True)\n",
      "    total_freq = coha_words.freq.sum()\n",
      "    median_counter = total_freq / 2\n",
      "    median_found = False\n",
      "    for idx, row in coha_words.iterrows():\n",
      "        curr_len = len(coha_words.word[idx])\n",
      "        coha_words.length[idx] = curr_len\n",
      "        coha_words.decades[idx] = coha_words.decadesxlen[idx] / coha_words.length[idx]\n",
      "        weighted_length += coha_words.freq[idx] * curr_len\n",
      "        median_counter -= curr_len * coha_words.freq[idx]\n",
      "        if median_counter < 0 and median_found == False:\n",
      "            wt_median_len = curr_len\n",
      "            median_found = True\n",
      "    coha_words = coha_words[['word', 'freq', 'length', 'decades']]\n",
      "    coha_words.to_pickle(pickle_full_path)\n",
      "    wt_mean_len = weighted_length * 1.0 / total_freq\n",
      "    pickle.dump((wt_mean_len, wt_median_len), open(mean_median_full_path, \"w\"))\n",
      "    \n",
      "else:\n",
      "    print 'Reading pickle.'\n",
      "    coha_words = pd.read_pickle(pickle_full_path)\n",
      "    wt_mean_len, wt_median_len = pickle.load(open(mean_median_full_path, \"r\"))\n",
      "    \n",
      "print \"Weighted mean word length: \" + str(wt_mean_len)\n",
      "print \"Weighted median word length: \" + str(wt_median_len)\n",
      "print \"Tail of coha_words dataframe:\"\n",
      "print coha_words.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading pickle.\n",
        "Weighted mean word length: 4.45220167708"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Weighted median word length: 4\n",
        "Tail of coha_words dataframe:\n",
        "            word  freq  length  decades\n",
        "336193      zieg     1       4        1\n",
        "87532   eckerson     1       8        1\n",
        "125546   haldore     1       7        1\n",
        "303578    tranio     1       6        1\n",
        "40616       burz     1       4        1\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create list of coha_words that also appears in moby crossword dictionary\n",
      "\n",
      "xword = set()\n",
      "with open(init_path + '/word_list_moby_crossword.flat.txt', 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        xword.add(line.strip())\n",
      "\n",
      "cohawords = set(coha_words.word)\n",
      "\n",
      "coha_moby = xword.intersection(cohawords)\n",
      "\n",
      "print \"{0} words out of intersection of {1} and {2}\".format(len(coha_moby), len(cohawords), len(xword))\n",
      "\n",
      "open(data_path + '/coha_and_xword.json', 'w+').write(json.dumps(list(coha_moby)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "78280 words out of intersection of 337085 and 113809\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Brown corpus from NLTK 3##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "#nltk.download() # uncomment to start downloader window to fetch brown or other corpora\n",
      "\n",
      "data_path = 'data'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_brown_initial = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/brown_df.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_brown_initial == True: \n",
      "    \n",
      "    print \"Processing Brown corpus from NLTK\"\n",
      "    categories = []\n",
      "    words = []\n",
      "    frequencies = []\n",
      "    \n",
      "    for category in brown.categories():\n",
      "        wordlist = brown.words(categories=category)\n",
      "        freqs = nltk.FreqDist([w.lower() for w in wordlist])\n",
      "        for key in freqs.keys():\n",
      "            categories.append(category)\n",
      "            words.append(key)\n",
      "            frequencies.append(freqs[key])\n",
      "    \n",
      "    brown_df = pd.DataFrame({'word':words, 'freq':frequencies, 'category':categories})\n",
      "    brown_df['nonalpha'] = False\n",
      "    brown_df['nonalpha'][brown_df.word.str.contains('[^A-Za-z]')] = True\n",
      "    brown_df.to_pickle(pickle_full_path)\n",
      "    \n",
      "else:\n",
      "    print \"Reading \" + pickle_full_path\n",
      "    brown_df = pd.read_pickle(pickle_full_path)\n",
      "    \n",
      "print \"\\nTail of dataframe:\"\n",
      "print brown_df.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing Brown corpus from NLTK\n",
        "\n",
        "Tail of dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               category  freq        word nonalpha\n",
        "139440  science_fiction     1        yoga    False\n",
        "139441  science_fiction     1      you'll     True\n",
        "139442  science_fiction     1    yourself    False\n",
        "139443  science_fiction     1  zigzagging    False\n",
        "139444  science_fiction     1        zone    False\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "build dataframe of frequencies of unique words, consolidating categories, discarding tokens with non-alphabetic characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_brown_words = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"brown_words.pickle\"\n",
      "\n",
      "mean_median_full_path = data_path + \"brown_words_mean_median.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_brown_words == True:\n",
      "    print 'Creating dataframe.'\n",
      "    total_freq = 0\n",
      "    weighted_length = 0\n",
      "    brown_words = brown_df[brown_df.nonalpha == 0]\n",
      "    brown_words = pd.DataFrame(brown_words.groupby(['word']).sum()).reset_index(drop=False)\n",
      "    brown_words['length'] = 0\n",
      "    brown_words.sort('freq', ascending = False, inplace=True)\n",
      "    total_freq = brown_words.freq.sum()\n",
      "    median_counter = total_freq / 2\n",
      "    median_found = False\n",
      "    for idx, row in brown_words.iterrows():\n",
      "        curr_len = len(brown_words.word[idx])\n",
      "        brown_words.length[idx] = curr_len\n",
      "        weighted_length += brown_words.freq[idx] * curr_len\n",
      "        median_counter -= curr_len * brown_words.freq[idx]\n",
      "        if median_counter < 0 and median_found == False:\n",
      "            wt_median_len = curr_len\n",
      "            median_found = True\n",
      "    brown_words = brown_words[['word', 'freq', 'length']]\n",
      "    brown_words.to_pickle(pickle_full_path)\n",
      "    wt_mean_len = weighted_length * 1.0 / total_freq\n",
      "    pickle.dump((wt_mean_len, wt_median_len), open(mean_median_full_path, \"w\"))\n",
      "    \n",
      "else:\n",
      "    print 'Reading pickle.'\n",
      "    brown_words = pd.read_pickle(pickle_full_path)\n",
      "    wt_mean_len, wt_median_len = pickle.load(open(mean_median_full_path, \"r\"))\n",
      "    \n",
      "print \"Weighted mean word length: \" + str(wt_mean_len)\n",
      "print \"Weighted median word length: \" + str(wt_median_len)\n",
      "print \"Tail of brown_words dataframe:\"\n",
      "print brown_words.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating dataframe.\n",
        "Weighted mean word length: 4.68324851586"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Weighted median word length: 4\n",
        "Tail of brown_words dataframe:\n",
        "                   word  freq  length\n",
        "11805           enchant     1       7\n",
        "11804         enchained     1       9\n",
        "11803  encephalographic     1      16\n",
        "11802      encephalitis     1      12\n",
        "29339           regalia     1       7\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Europarl ##\n",
      "\n",
      "#### Selection of 20 different languages from European Parliament ####\n",
      "\n",
      "The corpora for parallel translations are used because they are better curated than the raw texts. The English corpus is a combination of the English versions of the parallel translations for the languages with the most (not necessarily native) speakers in the world."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "import nltk\n",
      "import re\n",
      "\n",
      "europarl_filenames = {'Spanish':'europarl-v7.es-en.es', 'French':'europarl-v7.fr-en.fr',\n",
      "                      'German':'europarl-v7.de-en.de', 'Portuguese':'europarl-v7.pt-en.pt', \n",
      "                      'Italian':'europarl-v7.it-en.it', 'Polish':'europarl-v7.pl-en.pl',\n",
      "                      'Finnish':'europarl-v7.fi-en.fi', 'English':'europarl-v7.fr-en.en'} \n",
      "\n",
      "# this is not complete; notably missing are Hungarian diacritics\n",
      "accents = [(r'\u0104', r'A'), (r'\u0105', r'a'), (r'\u010c', r'C'), (r'\u010d', r'c'), (r'\u010f', r'd'), (r'\u0118', r'E'), (r'\u0119', r'e'), \n",
      "           (r'\u011a', r'E'), (r'\u011b', r'e'), (r'\u0139', r'L'), (r'\u013a', r'l'), (r'\u0147', r'N'), (r'\u0148', r'n'), (r'\u0154', r'R'), \n",
      "           (r'\u0155', r'r'), (r'\u0158', r'R'), (r'\u0159', r'r'), (r'\u0165', r't'), (r'\u016e', r'r'), (r'\u016f', r'r'), (r'\u017d', r'Z'), \n",
      "           (r'\u017e', r'z'), (r'\u00c1', r'A'), (r'\u00e1', r'a'), (r'\u00c2', r'A'), (r'\u00e2', r'a'), (r'\u00d8', r'o'), (r'\u00f5', r'o'), \n",
      "           (r'\u00c0', r'A'), (r'\u00e0', r'a'), (r'\u00c4', r'A'), (r'\u00e4', r'a'), (r'\u00c7', r'C'), (r'\u00e7', r'c'), (r'\u00c9', r'E'), \n",
      "           (r'\u00e9', r'e'), (r'\u00ca', r'E'), (r'\u00ea', r'e'), (r'\u00c8', r'E'), (r'\u00e8', r'e'), (r'\u00cb', r'E'), (r'\u00eb', r'e'), \n",
      "           (r'\u00cd', r'I'), (r'\u00ed', r'i'), (r'\u00ce', r'I'), (r'\u00ee', r'i'), (r'\u00cc', r'I'), (r'\u00ec', r'i'), (r'\u00cf', r'I'), \n",
      "           (r'\u00ef', r'i'), (r'\u00d1', r'N'), (r'\u00f1', r'n'), (r'\u00d3', r'O'), (r'\u00f3', r'o'), (r'\u00d4', r'O'), (r'\u00f4', r'o'), \n",
      "           (r'\u00d2', r'O'), (r'\u00f2', r'o'), (r'\u00d6', r'O'), (r'\u00f6', r'o'), (r'\u00f8', r'o'), (r'\u00d5', r'O'), (r'\u00e4', r'a'),\n",
      "           (r'\u00da', r'r'), (r'\u00fa', r'r'), (r'\u00db', r'r'), (r'\u00fb', r'r'), (r'\u00d9', r'r'), (r'\u00f9', r'r'), (r'\u00dc', r'r'), \n",
      "           (r'\u00fc', r'r'), (r'\u00dd', r'Y'), (r'\u00fd', r'y'), (r'\u0160', r'S'), (r'\u0161', r's'), (r'\u00ff', r'y'), (r'\u0178', r'Y'), \n",
      "           (r'\u00c5', r'A'), (r'\u00e5', r'a'), (r'\u00c3', r'A'), (r'\u00e3', r'a'), (r'\u00c4', r'A'), \n",
      "           (r'\u00c6', r'Ae'), (r'\u00e6', r'ae'), (r'\u0152', r'Oe'), (r'\u0153', r'oe'), (r'\u00df', r'ss')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_europarl = True # change to True to redo munge and rewrite pickle\n",
      "\n",
      "import time\n",
      "start = time.time()\n",
      "\n",
      "language = 'English' # This takes a long time -- Finnish was the longest, at 10.3 hours with a\n",
      "                     # $500 Windows Acer desktop computer bought in 2013. Therefore I munged the files\n",
      "                     # one by one instead of iterating over a list of them.\n",
      "\n",
      "if language == 'English':\n",
      "\n",
      "    file_path = init_path + europarl_filenames[language]\n",
      "    pickle_path = data_path + 'europarl_' + language + \".pickle\"\n",
      "    \n",
      "    if not os.path.isfile(pickle_path) or redo_europarl == True: \n",
      "        \n",
      "        print \"\\nProcessing\", language\n",
      "        \n",
      "        txt = open(file_path, 'rU').read()\n",
      "        freqdist = nltk.FreqDist(nltk.word_tokenize(txt))\n",
      "        \n",
      "        europarl_df = pd.DataFrame()\n",
      "\n",
      "        counter_max = len(freqdist.keys())\n",
      "        counter_list = []\n",
      "        for i in range(20):\n",
      "            counter_list.append(int(i * counter_max / 20))\n",
      "        counter = -1\n",
      "        countdown = 100\n",
      "        \n",
      "        for token in freqdist.keys():\n",
      "            counter += 1\n",
      "            if counter in counter_list:\n",
      "                print countdown,\n",
      "                countdown -= 5\n",
      "            token_mod = token.lower()\n",
      "            for accent in accents:\n",
      "                token_mod = re.sub(accent[0], accent[1], token_mod)\n",
      "            if len(token_mod) > 1 and re.search('[aeiouy]', token_mod) and not re.search('[^a-z]', token_mod):\n",
      "                df_to_append = pd.DataFrame({'language':[language], 'word':[token_mod], 'freq':[freqdist[token]]})\n",
      "                europarl_df = europarl_df.append(df_to_append)\n",
      "        \n",
      "        europarl_df.to_pickle(pickle_path)\n",
      "        \n",
      "        print \"\\n%d minutes elapsed.\" % (int((time.time() - start) / 60))\n",
      "        \n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing English\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 95 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "90 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "85 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "80 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "75 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "65 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "60 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "55 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "45 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "35 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18 minutes elapsed.\n",
        "Done.\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "initial_data_munge.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# word_list_tools repo\n",
      "\n",
      "* by David Taylor, www.prooffreader.com, prooffreader@gmail.com\n",
      "* a collection of tools to create and analyze lists of words using python with pandas and matplotlib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## initial_data_munge\n",
      "\n",
      "* Creates complete and summary dataframes\n",
      "* two word databases implemented:\n",
      "    1. COHA (not public domain, requires license to use)\n",
      "    2. Brown corpus (public domain, part of the python Natural Language Toolkit, NLTK).\n",
      "* The scripts should be easily adaptable to other lists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### COHA ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import pickle\n",
      "\n",
      "data_path = 'data'\n",
      "data_filename = '1_pos_n_cs_n_academicUse.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create \"coha\" initial dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_initial = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/coha.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_initial == True: # if pickle already exists, coha is not processed, only read\n",
      "    print \"Processing \" + coha_filename\n",
      "    coha = pd.read_table(data_path + \"/\" + data_filename)\n",
      "    print \"\\nTail of initial dataframe:\"\n",
      "    print coha.tail()\n",
      "    coha.columns = ['freq', 'word', 'decade']\n",
      "    coha = coha[['word', 'freq', 'decade']]\n",
      "    coha = coha.fillna('nan')\n",
      "    coha['nonalpha'] = False\n",
      "    coha['nonalpha'][coha.word.str.contains('[^A-Za-z]')] = True\n",
      "    coha['length'] = 0\n",
      "    decades = [1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900,\n",
      "               1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000]\n",
      "    for idx, row in coha.iterrows():  ## this step take 2-3 minutes on a medium-quality desktop computer ca 2013\n",
      "        coha['decade'][idx] = decades[coha['decade'][idx] - 1]\n",
      "        coha['length'][idx] = len(str(coha.word[idx]))\n",
      "    def coha_pct(group):\n",
      "        freq = group.freq.astype(float)\n",
      "        group['pct'] = (freq / freq.sum() * 100)\n",
      "        return group\n",
      "    coha = coha.groupby(['decade']).apply(coha_pct)\n",
      "    coha.to_pickle(pickle_full_path)\n",
      "    \n",
      "else:\n",
      "    print \"Reading \" + pickle_full_path\n",
      "    coha = pd.read_pickle(pickle_full_path)\n",
      "    \n",
      "print \"\\nTail of dataframe:\"\n",
      "print coha.tail()\n",
      "\n",
      "# note: 'nonalpha' is True if word contains any characters but a-z or A-Z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading data/coha.pickle\n",
        "\n",
        "Tail of dataframe:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              word  freq  decade nonalpha  length       pct\n",
        "2539723       zzzz     3    2000    False       4  0.000011\n",
        "2539724     zzzzzz     1    1980    False       6  0.000004\n",
        "2539725     zzzzzz     1    2000    False       6  0.000004\n",
        "2539726   zzzzzzzz     3    2000    False       8  0.000011\n",
        "2539727  zzzzzzzzz     3    2000    False       9  0.000011\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redo_coha_words = False # change to True to redo munge and rewrite pickle\n",
      "\n",
      "pickle_full_path = data_path + \"/coha_words.pickle\"\n",
      "mean_median_full_path = data_path + \"coha_words_mean_median.pickle\"\n",
      "\n",
      "if not os.path.isfile(pickle_full_path) or redo_coha_words == True:\n",
      "    print 'Creating dataframe.'\n",
      "    total_freq = 0\n",
      "    weighted_length = 0\n",
      "    coha_words = coha[coha.nonalpha == 0]\n",
      "    coha_words = pd.DataFrame(coha_words.groupby(['word']).sum()).reset_index(drop=False)\n",
      "    coha_words.columns = ['word', 'freq', 'erase1', 'erase2', 'decadesxlen', 'erase3']\n",
      "    coha_words['length'] = 0\n",
      "    coha_words['decades'] = 0\n",
      "    coha_words.sort('freq', ascending = False, inplace=True)\n",
      "    total_freq = coha_words.freq.sum()\n",
      "    median_target = total_freq / 2\n",
      "    freq_count = 0\n",
      "    for idx, row in coha_words.iterrows():\n",
      "        curr_len = len(coha_words.word[idx])\n",
      "        coha_words.length[idx] = curr_len\n",
      "        coha_words.decades[idx] = coha_words.decadesxlen[idx] / coha_words.length[idx]\n",
      "        weighted_length += coha_words.freq[idx] * curr_len\n",
      "        if (freq_count <= median_target and ###### doesn't work\n",
      "            freq_count + curr_len > median_target):\n",
      "            wt_median_len = curr_len\n",
      "            freq_count += curr_len\n",
      "    coha_words = coha_words[['word', 'freq', 'length', 'decades']]\n",
      "    coha_words.to_pickle(pickle_full_path)\n",
      "    wt_mean_len = weighted_length * 1.0 / total_freq\n",
      "    pickle.dump((wt_mean_len, wt_median_len), mean_median_full_path)\n",
      "    \n",
      "else:\n",
      "    print 'Reading pickle.'\n",
      "    coha_words = pd.read_pickle(pickle_full_path)\n",
      "    wt_mean_len, wt_median_len = pickle.load(mean_median_full_path)\n",
      "    \n",
      "print \"Weighted mean word length: \" + str(wt_mean_len)\n",
      "print \"Weighted median word length: \" + str(wt_median_len)\n",
      "print \"Tail of coha_words dataframe:\"\n",
      "print coha_words.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'wt_median_len' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-3463c06c5937>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcoha_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_full_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mwt_mean_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_length\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_freq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwt_mean_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwt_median_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_median_full_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'wt_median_len' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating dataframe.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wt_median_len"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'wt_median_len' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-1eac969d06fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwt_median_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'wt_median_len' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not os.path.isfile('decades_top.txt'):\n",
      "    # for each number of decades, find the top 10 words\n",
      "    f = open('decades_top.txt', 'a')\n",
      "    for dec in range(1,21):\n",
      "        print >> f, \"%d decade(s)\" % dec\n",
      "        print >> f, words[words.decades == dec].sort('freq', ascending=False).head(10)\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}